{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "personal-processor",
   "metadata": {},
   "source": [
    "# Training of the model step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-pencil",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-documentary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py as h5\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gwpy.timeseries import TimeSeries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, RepeatVector, Conv1D,\\\n",
    "                                    MaxPooling1D, UpSampling1D, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-drill",
   "metadata": {},
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_LSTM(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = LSTM(32, activation='relu', return_sequences=True, \n",
    "              kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(8, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = LSTM(8, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(32, activation='relu', return_sequences=True)(L4)\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "def autoencoder_GRU(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = GRU(32, activation='relu', return_sequences=True, \n",
    "              kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = GRU(8, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = GRU(8, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = GRU(32, activation='relu', return_sequences=True)(L4)\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "def autoencoder_Conv(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(inputs) # 10 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L2 = MaxPooling1D(4, padding=\"same\")(L1) # 5 dims\n",
    "    L3 = Conv1D(10, 3, activation=\"relu\", padding=\"same\")(L2) # 5 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling1D(4, padding=\"same\")(L3) # 3 dims\n",
    "    # 3 dimensions in the encoded layer\n",
    "    L4 = Conv1D(10, 3, activation=\"relu\", padding=\"same\")(encoded) # 3 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L5 = UpSampling1D(4)(L4) # 6 dims\n",
    "    L6 = Conv1D(16, 2, activation='relu')(L5) # 5 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L7 = UpSampling1D(4)(L6) # 10 dims\n",
    "    output = Conv1D(1, 3, activation='sigmoid', padding='same')(L7) # 10 dims\n",
    "    model = Model(inputs=inputs, outputs = output)\n",
    "    return model\n",
    "\n",
    "def autoencoder_Conv2(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = Conv1D(16, 4, activation=\"relu\", dilation_rate=1, padding=\"same\")(inputs)\n",
    "    L2 = MaxPooling1D(2)(L1)\n",
    "    L3 = Conv1D(32, 4, activation=\"relu\", dilation_rate=2, padding=\"same\")(L2)\n",
    "    L4 = MaxPooling1D(2)(L3) \n",
    "    L5 = Conv1D(64, 4, activation=\"relu\", dilation_rate=2, padding=\"same\")(L4)\n",
    "    L6 = MaxPooling1D(4)(L5)\n",
    "    L7 = Conv1D(128, 8, activation=\"relu\", dilation_rate=2, padding=\"same\")(L6)\n",
    "    encoded = MaxPooling1D(4)(L7)\n",
    "    L7 = Conv1D(128, 8, activation=\"relu\", dilation_rate=2, padding=\"same\")(encoded)\n",
    "    L8 = UpSampling1D(4)(L7)\n",
    "    L9 = Conv1D(64, 4, activation=\"relu\", dilation_rate=2, padding=\"same\")(L8)\n",
    "    L10 = UpSampling1D(4)(L9)\n",
    "    L11 = Conv1D(32, 4, activation=\"relu\", dilation_rate=2, padding=\"same\")(L10)\n",
    "    L12 = UpSampling1D(4)(L11)\n",
    "    L13 = Conv1D(16, 3, activation=\"relu\", dilation_rate=1, padding=\"same\")(L12)\n",
    "    L14 = UpSampling1D(2)(L13)\n",
    "    output = Conv1D(1, 4, activation=\"relu\", dilation_rate=1, padding=\"same\")(L12)\n",
    "    model = Model(inputs=inputs, outputs = output)\n",
    "    return model\n",
    "\n",
    "def autoencoder_ConvDNN(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(inputs) # 10 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L2 = MaxPooling1D(4, padding=\"same\")(L1) # 5 dims\n",
    "    L3 = Conv1D(10, 3, activation=\"relu\", padding=\"same\")(L2) # 5 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling1D(4, padding=\"same\")(L3) # 3 dims\n",
    "    x = Flatten()(encoded)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(10, activation='relu')(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(70, activation='relu')(x)\n",
    "    x = Reshape((7, 10))(x)\n",
    "    # 3 dimensions in the encoded layer\n",
    "    L4 = Conv1D(10, 3, activation=\"relu\", padding=\"same\")(x) # 3 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L5 = UpSampling1D(4)(L4) # 6 dims\n",
    "    L6 = Conv1D(16, 2, activation='relu')(L5) # 5 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L7 = UpSampling1D(4)(L6) # 10 dims\n",
    "    output = Conv1D(1, 3, activation='sigmoid', padding='same')(L7) # 10 dims\n",
    "    model = Model(inputs=inputs, outputs = output)\n",
    "    return model\n",
    "\n",
    "def autoencoder_ConvLSTM(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(inputs) # 10 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L2 = MaxPooling1D(4, padding=\"same\")(L1) # 5 dims\n",
    "    L3 = Conv1D(10, 3, activation=\"relu\", padding=\"same\")(L2) # 5 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling1D(4, padding=\"same\")(L3) # 3 dims\n",
    "    x = Reshape((70, 1))(encoded)\n",
    "    \n",
    "    x = LSTM(32, activation='relu', return_sequences=False, \n",
    "              kernel_regularizer=regularizers.l2(0.00))(x)\n",
    "    x = RepeatVector(70)(x)\n",
    "    x = LSTM(32, activation='relu', return_sequences=True)(x)\n",
    "    out = TimeDistributed(Dense(1))(x)  \n",
    "    \n",
    "    x = Reshape((7, 10))(out)\n",
    "    # 3 dimensions in the encoded layer\n",
    "    L4 = Conv1D(10, 3, activation=\"relu\", padding=\"same\")(x) # 3 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L5 = UpSampling1D(4)(L4) # 6 dims\n",
    "    L6 = Conv1D(32, 2, activation='relu')(L5) # 5 dims\n",
    "    #x = BatchNormalization()(x)\n",
    "    L7 = UpSampling1D(4)(L6) # 10 dims\n",
    "    output = Conv1D(1, 3, activation='sigmoid', padding='same')(L7) # 10 dims\n",
    "    model = Model(inputs=inputs, outputs = output)\n",
    "    return model\n",
    "\n",
    "def autoencoder_DNN(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(int(X.shape[1]/2), activation='relu')(x)\n",
    "    x = Dense(int(X.shape[1]/10), activation='relu')(x)\n",
    "    x = Dense(int(X.shape[1]/2), activation='relu')(x)\n",
    "    x = Dense(X.shape[1], activation='relu')(x)\n",
    "    output = Reshape((X.shape[1], 1))(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-bahrain",
   "metadata": {},
   "source": [
    "## Define some I/O variables\n",
    "\n",
    "1. Select one of the above defined models (autoencoder_LSTM, autoencoder_GRU, autoencoder_Conv, autoencoder_Conv2, autoencoder_ConvDNN, autoencoder_ConvLSTM, autoencoder_DNN).\n",
    "2. Select the LIGO detector, the sampling frequency of the detector, whether to filter the data and the # of timesteps to pass to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "model_chosen = 'autoencoder_LSTM'   #change this\n",
    "outdir = './output/' + model_chosen\n",
    "detector = 'L1'                     # LIGO detector. L1 or H1\n",
    "freq = '2'                          # Sampling frequency of the detector in KHz. 2 or 4\n",
    "filtered = '1'                      # Apply bandpass and whitening filters. 1/0 \n",
    "timesteps = 100                     # Number of timesteps passed to model\n",
    "\n",
    "# Create output directory\n",
    "os.system(f'mkdir {outdir}')\n",
    "\n",
    "# Define frequency in Hz instead of KHz\n",
    "if int(freq) == 2:\n",
    "    freq = 2048\n",
    "elif int(freq) == 4:\n",
    "    freq = 4096\n",
    "else:\n",
    "    print(f'Given frequency {freq}kHz is not supported. Correct values are 2 or 4kHz.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-accounting",
   "metadata": {},
   "source": [
    "## Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd())\n",
    "\n",
    "load = h5.File('./../dataset/dataset1.hdf', 'r')   # Change this to dataset location!\n",
    "print('File keys:\\n')\n",
    "print([key for key in load.keys()])\n",
    "\n",
    "group = load['noise_samples']\n",
    "print('\\n'+'Noise_samples keys:\\n')\n",
    "print([key for key in group.keys()])\n",
    "\n",
    "dataset = group[f'{str(detector).lower()}_strain'][0:6000]  # I use only one half of the dataset\n",
    "print('\\n'+'L1 strain shape:\\n')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-funeral",
   "metadata": {},
   "source": [
    "### Strain plot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = group['l1_strain'][0]\n",
    "print(y.shape)\n",
    "plt.plot(y[0:200]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-stupid",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-passenger",
   "metadata": {},
   "source": [
    "### Apply whitening and bandpass\n",
    "\n",
    "With LIGO simulated data, the sample isn't pre-filtered so need to filter.\n",
    "\n",
    "(**???** The gwpy package generates filtered data.\n",
    "In particular it applies a whitening procedure and a bandpass, **but** it uses different lower and upper bounds (20 Hz & 2048 Hz - see the file `waveform_params.ini`)).\n",
    "\n",
    "Anyway, here I apply the filtering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filters(array, sample_frequency):\n",
    "    \"\"\" Apply preprocessing such as whitening and bandpass \"\"\"\n",
    "    strain = TimeSeries(array, sample_rate=int(sample_frequency))   # Create an instance of the class TimeSeries\n",
    "    white_data = strain.whiten(fftlength=4, fduration=4)            # Whiten the TimeSeries. fftlength = fft integration length in s. fduration = duration in s of the filter\n",
    "    bp_data = white_data.bandpass(50, 250)                          # Band-pass filter\n",
    "    return bp_data.value                                            # .value is the numerical value of the instance\n",
    "\n",
    "if bool(int(filtered)):\n",
    "    print('Filtering data with whitening and bandpass')\n",
    "    print(f'Sample Frequency: {freq} Hz')\n",
    "    x = [filters(sample, freq) for sample in dataset]\n",
    "    print('Filtering completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-brooks",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(x)\n",
    "scaler_filename = f\"{outdir}/scaler_data_{detector}\"\n",
    "joblib.dump(scaler, scaler_filename)                      # Joblib.dump persists scaler into the file scaler_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-cartridge",
   "metadata": {},
   "source": [
    " ### Data augmentation\n",
    " \n",
    "This passage extends the dataset and is needed if there's not enough data.\n",
    "Here I skip it, because of hardware constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(X_train, timesteps):\n",
    "    x = []\n",
    "    for sample in X_train:\n",
    "        if sample.shape[0] % timesteps != 0:\n",
    "            corrected_sample = sample[:-1 * int(sample.shape[0] % timesteps)]    # Cut off the tail\n",
    "        sliding_sample = np.array([corrected_sample[i:i + timesteps][:] for i in [int(timesteps / 2) * n for n in range(int(len(corrected_sample) / (timesteps / 2)) - 1)]]) # window (width=timesteps) sliding on the cut sample\n",
    "        x.append(sliding_sample)\n",
    "    return np.array(x)\n",
    "\n",
    "# X_train = augmentation(X_train, timesteps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-electronics",
   "metadata": {},
   "source": [
    "### Trim and reshape\n",
    "\n",
    "Trim the dataset to be batch-friendly and reshape it into timestep format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[0].shape[0]\n",
    "print(f'Current dataset shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for event in range(len(X_train)):\n",
    "    if X_train[event].shape[0] % timesteps != 0:\n",
    "        x.append(X_train[event][:-1 * int(X_train[event].shape[0] % timesteps)])\n",
    "X_train = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset shape after trimming: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-method",
   "metadata": {},
   "source": [
    "Reshape inputs for LSTM [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "competent-career",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6699939a9052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Reshape the array w/o changing its data. The value for -1 is inferred form the other dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training data shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1, timesteps, 1)   # Reshape the array w/o changing its data. The value for -1 is inferred form the other dimensions\n",
    "print(\"Training data shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-balloon",
   "metadata": {},
   "source": [
    "## Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-chapter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = autoencoder_LSTM(X_train)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 10    # This was 300 in the original code!\n",
    "batch_size = 1024\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(f'{outdir}/best_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\n",
    "                        validation_split=0.2, callbacks=[early_stop, mcp_save]).history\n",
    "model.save(f'{outdir}/last_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mse)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.savefig(f'{outdir}/loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-galaxy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
